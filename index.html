<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Academic Project Page</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>

  <nav class="navbar" role="navigation" aria-label="main navigation">
    <div class="navbar-brand">
      <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
        <span aria-hidden="true"></span>
        <span aria-hidden="true"></span>
        <span aria-hidden="true"></span>
      </a>
    </div>
    <div class="navbar-menu">
      <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
        <div class="navbar-item has-dropdown is-hoverable">
          <a class="navbar-link">
            More Research
          </a>
          <div class="navbar-dropdown">
            <a class="navbar-item" href="https://tiger-ai-lab.github.io/MAmmoTH/">
              <b>MAmmoTH</b> <p style="font-size:18px; display: inline; margin-left: 5px;">ðŸ”¥</p>
            </a>
            <a class="navbar-item" href="https://osu-nlp-group.github.io/TableLlama/">
              TableLlama 
              <a class="navbar-item" href="https://osu-nlp-group.github.io/MagicBrush/">
                MagicBrush
              </a>
              <a class="navbar-item" href="https://osu-nlp-group.github.io/Mind2Web/">
                Mind2Web
              </a>
            </a>
            
            </a>
          </div>
        </div>
      </div>
  
    </div>
  </nav>

  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">GMAI-MMBench: A Comprehensive Multimodal
              Evaluation Benchmark Towards General Medical AI</h1>
              <div class="is-size-5 publication-authors">
                <span class="author-block">Pengcheng Chen*<sup style="color:#ffac33;">1,2</sup>,</span>
                <span class="author-block">Jin Ye*<sup style="color:#ed4b82;">1,3</sup>,</span>
                <span class="author-block">Guoan Wang<sup style="color:#007bff;">1,4</sup>,</span>
                <span class="author-block">Yanjun Li<sup style="color:#007bff;">1,4</sup>,</span><br>
                <span class="author-block">Zhongying Deng<sup style="color:#ffac33;">5</sup>,</span>
                <span class="author-block">Wei Li<sup style="color:#ed4b82;">1,6</sup>,</span>
                <span class="author-block">Tianbin Li<sup style="color:#ed4b82;">1</sup>,</span>
                <span class="author-block">Haodong Duan<sup style="color:#ffac33;">1</sup>,</span>
                <span class="author-block">Ziyan Huang<sup style="color:#ffac33;">1,6</sup>,</span>
                <span class="author-block">Yanzhou Su<sup style="color:#6fbf73;">1</sup>,</span>
                <span class="author-block">Benyou Wang<sup style="color:#9b51e0;">7,8</sup>,</span>
                <span class="author-block">Shaoting Zhang<sup style="color:#6fbf73;">1</sup>,</span>
                <span class="author-block">Bin Fu<sup style="color:#60121572;">9</sup>,</span>
                <span class="author-block">Jianfei Cai<sup style="color:#ed4b82;">3</sup>,</span>
                <span class="author-block">Bohan Zhuang<sup style="color:#ed4b82;">3</sup>,</span>
                <span class="author-block">Eric J Seibel<sup style="color:#ffac33;">2</sup>,</span>
                <span class="author-block">Junjun He<sup style="color:#6fbf73;">â€ ,1</sup>,</span>
                <span class="author-block">Yu Qiao<sup style="color:#6fbf73;">â€ ,1</sup>,</span>
              </div>
          
              <br>
          
              <div class="is-size-5 publication-authors">
                <span class="author-block"><sup style="color:#6fbf73;">1</sup>Shanghai AI Laboratory,</span>
                <span class="author-block"><sup style="color:#ffac33;">2</sup>University of Washington,</span>
                <span class="author-block"><sup style="color:#ed4b82;">3</sup>Monash University,</span>
                <span class="author-block"><sup style="color:#007bff;">4</sup>East China Normal University,</span></br>
                <span class="author-block"><sup style="color:#ffac33;">5</sup>University of Cambridge,</span>
                <span class="author-block"><sup style="color:#ed4b82;">6</sup>Shanghai Jiao Tong University,</span>
                <span class="author-block"><sup style="color:#9b51e0;">7</sup>The Chinese University of Hong Kong, Shenzhen</span>
                <span class="author-block"><sup style="color:#ff00f2;">8</sup>Shenzhen Research Institute of Big Data</span>
                <span class="author-block"><sup style="color:#60121572;">9</sup>Shenzhen Institute of Advanced Technology (SIAT), Chinese Academy of Sciences</span>
              </div>
    
              <br>
              <div class="is-size-5 publication-authors">
                <span class="author-block">*Core Contributors</span><br>
                <span class="author-block">â€ Corresponding to:</span>
                <span class="author-block"><a href="mailto:xiangyue.work@gmail.com">hejunjun@pjlab.org.cn</a>,</span>
                <span class="author-block"><a href="mailto:su.809@osu.edu">qiaoyu@pjlab.org.cn</a>,</span>
              </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://arxiv.org/pdf/<ARXIV PAPER ID>.pdf" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                    <!-- Supplementary PDF link -->
                    <span class="link-block">
                      <a href="static/pdfs/supplementary_material.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary</span>
                    </a>
                  </span>

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/YOUR REPO HERE" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/<ARXIV PAPER ID>" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="hero teaser">
  <div class="container is-max-desktop">
        <div class="content has-text-centered">
          <img src="static/images/cover.jpg" alt="geometric reasoning" width="100%"/>
          <p>  Overview of the GMAI-MMbench. GMAI-MMbench focuses on medical image understanding and reasoning in real-world clinical scenarios with three key features: (1) Comprehensive medical knowledge: It consists of 285 diverse clinical-related datasets from worldwide sources, covering 38 modalities. (2) Well-organized data structure: It features 19 clinical VQA tasks across 18 clinical departments, meticulously categorized into a lexical tree. (3) Multi-perceptual granularity: Interactive methods span from image to region level, offering varying degrees of perceptual details.
          </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container" style="margin-bottom: 2vh;">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">ðŸ””News</h2>
        <div class="content has-text-justified">
          <p>
            <b>ðŸš€[2024-06-05]: Submit on nips2024!ðŸŒŸ</b>
          </p>
      </div>      
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Large Vision-Language Models (LVLMs) are capable of handling diverse data types such as imaging, text, and physiological signals, and can be applied in various fields. In the medical field, LVLMs have a high potential to offer substantial assistance for diagnosis and treatment. Before that, it is crucial to develop benchmarks to evaluate LVLMs' effectiveness in various medical applications. Current benchmarks are often built upon academic literature, mainly focusing on a single domain, and lacking varying perceptual granularities. Thus, they face specific challenges, including limited clinical relevance, incomplete evaluations, and insufficient guidance for interactive LVLMs. To address these limitations, we developed GMAI-MMbench, the most comprehensive and fine-grained GMAI benchmark to date. It is constructed from 285 datasets across 38 medical image modalities, 19 clinical-related tasks, and 18 departments in a Visual Question Answering (VQA) format. Additionally, we implemented a lexical tree structure that allows users to customize evaluation tasks, accommodating various assessment needs and substantially supporting medical AI research and applications. We evaluated 50 LVLMs, and the results show that even the advanced GPT-4o only achieves an accuracy of 52\%, indicating significant room for improvement. Moreover, we identified 5 main insufficiencies to be addressed in the next-generation LVLMs. Addressing them can advance the development of cutting-edge LVLMs for medical applications. We believe GMAI-MMbench will stimulate the community to build the next generation of LVLMs toward GMAI.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
</div>
</section>

<!-- DATASET SECTION -->
<section class="hero is-light is-small">
  <div class="hero-body has-text-centered">
  <h1 class="title is-1 mmmu">
    <img src="static/images/mmmu_icon2.png" style="width:1em;vertical-align: middle" alt="Logo"/>
    <span class="mmmu" style="vertical-align: middle">GMAI-MMBench</span>
  </h1>
  </div>
</section>

<section class="section">
  <div class="container">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Overview</h2>
        <div class="content has-text-justified">
          <p>
            We propose GMAI-MMbench, an innovative benchmark meticulously designed for the medical field, capable of providing comprehensive evaluations of LVLMs across various aspects of healthcare. Our team collected 285 datasets from public sources and hospitals over one year, covering medical imaging tasks of detection, classification, and segmentation, to form the data fuel for establishing such a benchmark. The detailed datasets are listed in the supplementary. Based on the data foundation, we design a reliable pipeline to generate question-answering pairs and organize them across different aspects with manual validation. Finally, we carefully select approximately 26K questions with varying levels of perceptual granularity from the filtered cases to construct the final GMAI-MMbench.</p>
          <img src="static/images/workflow.png" alt="algebraic reasoning" class="center">
          <br>
          <p>
            Overall illustration of GMAI-MMbench. It can be divided into three core steps: 1) Data curation and preprocessing: We standardize both images and labels after curating 285 datasets. 2) Label categorization and lexical tree construction: We categorize all labels into 19 clinical VQA tasks and 18 clinical departments, then export a lexical tree for easily customized evaluation. 3) QA generation and selection: During the generation process, each question must include information about image modality and corresponding annotation granularity. The final benchmark is obtained through additional expert validation and manual selection.
          </p>
        </div>
    </div>
    </div>

    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Lexical Tree</h2>
        <div class="content has-text-justified">
          <p>
            In this work, to make the GMAI-MMBench more intuitive and user-friendly, we have systematized our labels and structured the entire dataset into a lexical tree. Users can freely select the test contents based on this lexical tree. We believe that this customizable benchmark will effectively guide the improvement of models in specific areas.
        </p>
        <div class="content has-text-centered">
          <img src="static/images/tree.png" alt="algebraic reasoning" class="center">
          <p> Sampled MMMU examples from each discipline. The questions and images need expert-level knowledge to understand and reason.</p>
        </div>
        </div>
    </div>
    </div>


    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Comparisons with Existing Benchmarks</h2>
        <div class="content has-text-justified">
          <p>
            To further distinguish the difference between <i>dataset</i> and other existing ones, we elaborate the benchmark details in Figure. 
            From the <i>breadth</i> perspective, the prior benchmarks are heavily focused on daily knowledge and common sense. 
            The covered image format is also limited. Our benchmark aims to cover college-level knowledge with 30 image formats including diagrams, 
            tables, charts, chemical structures, photos, paintings, geometric shapes, music sheets, medical images, etc. 
            In the <i>depth</i> aspect, the previous benchmarks normally require commonsense knowledge or simple physical or temporal reasoning. 
            In contrast, our benchmark requires deliberate reasoning with college-level subject knowledge.
        </p>
        <div class="content has-text-centered">
          <img src="static/images/compare.Jpeg" alt="algebraic reasoning" class="center">
          <p> Sampled MMMU examples from each discipline. The questions and images need expert-level knowledge to understand and reason.</p>
        </div>
        </div>
    </div>
    </div>

    <div class="columns is-centered m-6">
      <div class="column is-full has-text-centered content">
        <h2 class="title is-3">Statistics</h2>
        <div id="results-carousel" class="carousel results-carousel">
          <div class="box m-5">
            <div class="content has-text-centered">
              <img src="static/images/mmmu_subject_distribution.Jpeg" alt="algebraic reasoning" width="95%"/>
              <p> Sampled MMMU examples from each discipline. The questions and images need expert-level knowledge to understand and reason.</p>
            </div>
          </div>
          <div class="box m-5">
            <div class="content has-text-centered">
              <img src="static/images/statistics.png" alt="arithmetic reasoning" width="40%"/>
              <p> Key statistics of the MMMU benchmark</p>
            </div>
          </div>
          <div class="box m-5">
            <div class="content has-text-centered">
              <img src="static/images/image_type_count.png" alt="arithmetic reasoning" width="80%"/>
              <p> Distribution of image types in the MMMU dataset</p>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>





<!-- Paper poster -->
<!-- <section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title">Poster</h2>

      <iframe  src="static/pdfs/sample.pdf" width="100%" height="550">
          </iframe>
        
      </div>
    </div>
  </section> -->
<!--End paper poster -->


<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>BibTex Code Here</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from theÂ <a href="https://nerfies.github.io" target="_blank">Nerfies</a>Â project page.
            You are free to borrow the of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
